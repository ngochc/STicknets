{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a36ddb",
   "metadata": {},
   "source": [
    "# STickNet Feature Map Visualization\n",
    "\n",
    "This notebook demonstrates how to extract and visualize feature maps from different stages of the STickNet (Spatial Tick Network) model. We'll explore what the network learns at various depths by examining the intermediate representations.\n",
    "\n",
    "## Overview\n",
    "- Load and configure a STickNet model\n",
    "- Extract feature maps from different stages of the backbone\n",
    "- Create visualizations to understand learned features\n",
    "- Compare feature evolution across network stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d92823",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import Required Libraries\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransforms\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Add parent directory to path for importing models\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import STickNet components\n",
    "try:\n",
    "  from models import build_STickNet, SpatialTickNet\n",
    "  print(\"Successfully imported STickNet components\")\n",
    "except ImportError as e:\n",
    "  print(f\"Import error: {e}\")\n",
    "  # Fallback method\n",
    "  import importlib.util\n",
    "  spec = importlib.util.spec_from_file_location(\"STickNet\", \"../models/STickNet.py\")\n",
    "  STickNet_module = importlib.util.module_from_spec(spec)\n",
    "  spec.loader.exec_module(STickNet_module)\n",
    "  \n",
    "  build_STickNet = STickNet_module.build_STickNet\n",
    "  SpatialTickNet = STickNet_module.SpatialTickNet\n",
    "  print(\"Using fallback import method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c721bf",
   "metadata": {},
   "source": [
    "## Load and Configure STickNet Model\n",
    "Create a STickNet model and set it to evaluation mode for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a2531c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Build STickNet model\n",
    "model = build_STickNet(\n",
    "  num_classes=1000,  # ImageNet classes\n",
    "  typesize='small',\n",
    "  cifar=False,\n",
    "  use_lightweight_optimization=False\n",
    ")\n",
    "\n",
    "# Move model to device and set to evaluation mode\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ab0cea",
   "metadata": {},
   "source": [
    "## Prepare Input Data\n",
    "Load and preprocess input images to match model requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943841ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define transforms for preprocessing\n",
    "transform = transforms.Compose([\n",
    "  transforms.Resize((224, 224)),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create a sample input or load an image\n",
    "# Option 1: Create a random input for testing\n",
    "sample_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "\n",
    "# Option 2: Load a real image (uncomment if you have an image file)\n",
    "# image_path = \"path/to/your/image.jpg\"\n",
    "# if os.path.exists(image_path):\n",
    "#     image = Image.open(image_path).convert('RGB')\n",
    "#     sample_input = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "print(f\"Input tensor shape: {sample_input.shape}\")\n",
    "\n",
    "# Display input if it's a real image\n",
    "plt.figure(figsize=(6, 6))\n",
    "if sample_input.max() > 1:  # If not normalized\n",
    "  plt.imshow(sample_input.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "else:  # If normalized, denormalize for display\n",
    "  mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "  std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "  denorm_img = sample_input.squeeze().cpu() * std + mean\n",
    "  denorm_img = torch.clamp(denorm_img, 0, 1)\n",
    "  plt.imshow(denorm_img.permute(1, 2, 0).numpy())\n",
    "plt.title(\"Input Image\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d25ccc",
   "metadata": {},
   "source": [
    "## Extract Feature Maps from Different Stages\n",
    "Set up forward hooks to capture intermediate feature maps from the STickNet backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e9c0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Dictionary to store feature maps\n",
    "feature_maps = {}\n",
    "\n",
    "def get_activation(name):\n",
    "  \"\"\"Hook function to capture activations.\"\"\"\n",
    "  def hook(model, input, output):\n",
    "    feature_maps[name] = output.detach()\n",
    "  return hook\n",
    "\n",
    "# Register hooks for different stages\n",
    "hooks = []\n",
    "\n",
    "# Hook for initial convolution\n",
    "hooks.append(model.backbone.init_conv.register_forward_hook(get_activation('init_conv')))\n",
    "\n",
    "# Hook for each stage\n",
    "for i in range(5):  # STickNet has 5 stages\n",
    "  stage_name = f'stage{i+1}'\n",
    "  if hasattr(model.backbone, stage_name):\n",
    "    stage = getattr(model.backbone, stage_name)\n",
    "    hooks.append(stage.register_forward_hook(get_activation(stage_name)))\n",
    "\n",
    "# Hook for final conv\n",
    "hooks.append(model.backbone.final_conv.register_forward_hook(get_activation('final_conv')))\n",
    "\n",
    "# Forward pass to collect feature maps\n",
    "with torch.no_grad():\n",
    "  output = model(sample_input)\n",
    "\n",
    "print(\"Captured feature maps:\")\n",
    "for name, feature_map in feature_maps.items():\n",
    "  print(f\"{name}: {feature_map.shape}\")\n",
    "\n",
    "# Clean up hooks\n",
    "for hook in hooks:\n",
    "  hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7892f1df",
   "metadata": {},
   "source": [
    "## Create Feature Map Visualization Functions\n",
    "Implement helper functions to normalize and display feature maps effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd5b4c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def normalize_feature_map(feature_map):\n",
    "  \"\"\"Normalize feature map for visualization.\"\"\"\n",
    "  fm = feature_map.cpu().numpy()\n",
    "  fm_min, fm_max = fm.min(), fm.max()\n",
    "  fm_norm = (fm - fm_min) / (fm_max - fm_min + 1e-8)\n",
    "  return fm_norm\n",
    "\n",
    "def visualize_feature_maps(feature_maps_dict, layer_name, max_channels=16):\n",
    "  \"\"\"\n",
    "  Visualize feature maps from a specific layer.\n",
    "  \n",
    "  Args:\n",
    "    feature_maps_dict: Dictionary containing feature maps\n",
    "    layer_name: Name of the layer to visualize\n",
    "    max_channels: Maximum number of channels to display\n",
    "  \"\"\"\n",
    "  if layer_name not in feature_maps_dict:\n",
    "    print(f\"Layer {layer_name} not found in feature maps\")\n",
    "    return\n",
    "  \n",
    "  feature_map = feature_maps_dict[layer_name]\n",
    "  batch_size, channels, height, width = feature_map.shape\n",
    "  \n",
    "  # Limit number of channels to display\n",
    "  num_channels = min(channels, max_channels)\n",
    "  cols = 4\n",
    "  rows = (num_channels + cols - 1) // cols\n",
    "  \n",
    "  plt.figure(figsize=(15, 3 * rows))\n",
    "  plt.suptitle(f'Feature Maps from {layer_name} (Shape: {feature_map.shape})', fontsize=16)\n",
    "  \n",
    "  for i in range(num_channels):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    fm = normalize_feature_map(feature_map[0, i])\n",
    "    plt.imshow(fm, cmap='viridis')\n",
    "    plt.title(f'Channel {i}')\n",
    "    plt.axis('off')\n",
    "  \n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "def create_feature_map_summary(feature_maps_dict):\n",
    "  \"\"\"Create a summary visualization of all feature maps.\"\"\"\n",
    "  plt.figure(figsize=(20, 12))\n",
    "  \n",
    "  layer_names = list(feature_maps_dict.keys())\n",
    "  num_layers = len(layer_names)\n",
    "  cols = 3\n",
    "  rows = (num_layers + cols - 1) // cols\n",
    "  \n",
    "  for i, layer_name in enumerate(layer_names):\n",
    "    feature_map = feature_maps_dict[layer_name]\n",
    "    \n",
    "    # Take mean across channels for summary view\n",
    "    if len(feature_map.shape) == 4:\n",
    "      mean_fm = torch.mean(feature_map[0], dim=0).cpu().numpy()\n",
    "    else:\n",
    "      mean_fm = feature_map[0].cpu().numpy()\n",
    "    \n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    \n",
    "    if len(mean_fm.shape) == 2:\n",
    "      plt.imshow(normalize_feature_map(torch.tensor(mean_fm)), cmap='viridis')\n",
    "    else:\n",
    "      # For 1D features (like global pool output)\n",
    "      plt.plot(mean_fm.flatten())\n",
    "    \n",
    "    plt.title(f'{layer_name}\\nShape: {feature_map.shape}')\n",
    "    plt.axis('off')\n",
    "  \n",
    "  plt.suptitle('Feature Map Summary Across All Layers', fontsize=16)\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3abdc5b",
   "metadata": {},
   "source": [
    "## Visualize Initial Convolution Features\n",
    "Display feature maps from the initial convolution layer to observe low-level features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa44bf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Visualize initial convolution features\n",
    "visualize_feature_maps(feature_maps, 'init_conv', max_channels=16)\n",
    "\n",
    "print(\"Initial convolution layer captures low-level features like edges and textures.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f85001",
   "metadata": {},
   "source": [
    "## Visualize Stage-wise Feature Maps\n",
    "Explore how features evolve through each stage of the STickNet backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce99833",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Visualize feature maps from each stage\n",
    "stages = ['stage1', 'stage2', 'stage3', 'stage4', 'stage5']\n",
    "\n",
    "for stage in stages:\n",
    "  if stage in feature_maps:\n",
    "    print(f\"\\n=== {stage.upper()} ===\")\n",
    "    visualize_feature_maps(feature_maps, stage, max_channels=12)\n",
    "    \n",
    "    # Print stage analysis\n",
    "    fm = feature_maps[stage]\n",
    "    print(f\"Stage shape: {fm.shape}\")\n",
    "    print(f\"Spatial resolution: {fm.shape[2]}x{fm.shape[3]}\")\n",
    "    print(f\"Number of channels: {fm.shape[1]}\")\n",
    "    print(f\"Feature range: [{fm.min().item():.3f}, {fm.max().item():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934245d8",
   "metadata": {},
   "source": [
    "## Create Multi-stage Feature Comparison\n",
    "Generate side-by-side comparisons to observe feature evolution across stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4272ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create comprehensive feature map summary\n",
    "create_feature_map_summary(feature_maps)\n",
    "\n",
    "# Compare specific channels across stages\n",
    "def compare_across_stages(feature_maps_dict, channel_idx=0):\n",
    "  \"\"\"Compare a specific channel across different stages.\"\"\"\n",
    "  stages_to_compare = ['init_conv', 'stage1', 'stage2', 'stage3', 'stage4', 'stage5']\n",
    "  \n",
    "  plt.figure(figsize=(18, 3))\n",
    "  \n",
    "  for i, stage in enumerate(stages_to_compare):\n",
    "    if stage in feature_maps_dict:\n",
    "      fm = feature_maps_dict[stage]\n",
    "      if fm.shape[1] > channel_idx:  # Check if channel exists\n",
    "        plt.subplot(1, len(stages_to_compare), i + 1)\n",
    "        channel_fm = normalize_feature_map(fm[0, channel_idx])\n",
    "        plt.imshow(channel_fm, cmap='viridis')\n",
    "        plt.title(f'{stage}\\nCh {channel_idx}\\n{fm.shape[2]}x{fm.shape[3]}')\n",
    "        plt.axis('off')\n",
    "  \n",
    "  plt.suptitle(f'Feature Evolution Across Stages (Channel {channel_idx})', fontsize=14)\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "# Compare first channel across stages\n",
    "compare_across_stages(feature_maps, channel_idx=0)\n",
    "\n",
    "# Compare different channels if available\n",
    "if any(fm.shape[1] > 5 for fm in feature_maps.values()):\n",
    "  compare_across_stages(feature_maps, channel_idx=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eb4086",
   "metadata": {},
   "source": [
    "## Save Feature Map Visualizations\n",
    "Export the generated visualizations for documentation and further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061360dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "output_dir = '../feature_maps_output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def save_feature_map_visualization(feature_maps_dict, layer_name, output_dir, max_channels=16):\n",
    "  \"\"\"Save feature map visualization to file.\"\"\"\n",
    "  if layer_name not in feature_maps_dict:\n",
    "    return\n",
    "  \n",
    "  feature_map = feature_maps_dict[layer_name]\n",
    "  batch_size, channels, height, width = feature_map.shape\n",
    "  \n",
    "  num_channels = min(channels, max_channels)\n",
    "  cols = 4\n",
    "  rows = (num_channels + cols - 1) // cols\n",
    "  \n",
    "  plt.figure(figsize=(15, 3 * rows))\n",
    "  plt.suptitle(f'Feature Maps from {layer_name} (Shape: {feature_map.shape})', fontsize=16)\n",
    "  \n",
    "  for i in range(num_channels):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    fm = normalize_feature_map(feature_map[0, i])\n",
    "    plt.imshow(fm, cmap='viridis')\n",
    "    plt.title(f'Channel {i}')\n",
    "    plt.axis('off')\n",
    "  \n",
    "  plt.tight_layout()\n",
    "  plt.savefig(os.path.join(output_dir, f'{layer_name}_feature_maps.png'), \n",
    "              dpi=300, bbox_inches='tight')\n",
    "  plt.close()\n",
    "  print(f\"Saved {layer_name} feature maps to {output_dir}\")\n",
    "\n",
    "# Save visualizations for all layers\n",
    "for layer_name in feature_maps.keys():\n",
    "  save_feature_map_visualization(feature_maps, layer_name, output_dir)\n",
    "\n",
    "# Save the summary comparison\n",
    "plt.figure(figsize=(20, 12))\n",
    "create_feature_map_summary(feature_maps)\n",
    "plt.savefig(os.path.join(output_dir, 'feature_maps_summary.png'), \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nAll feature map visualizations saved to: {output_dir}\")\n",
    "print(\"Files saved:\")\n",
    "for file in os.listdir(output_dir):\n",
    "  if file.endswith('.png'):\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d987c064",
   "metadata": {},
   "source": [
    "## Analysis and Conclusions\n",
    "\n",
    "### Key Observations:\n",
    "\n",
    "1. **Initial Convolution**: Captures low-level features like edges, textures, and basic patterns\n",
    "2. **Early Stages (Stage 1-2)**: Focus on local feature detection and simple pattern recognition\n",
    "3. **Middle Stages (Stage 3-4)**: Combine local features into more complex patterns and shapes\n",
    "4. **Late Stages (Stage 5)**: High-level semantic features relevant for classification\n",
    "5. **Final Convolution**: Abstract representations ready for classification\n",
    "\n",
    "### STickNet Architecture Insights:\n",
    "\n",
    "- **Spatial Attention**: The SE attention mechanism helps focus on important spatial regions\n",
    "- **Feature Evolution**: Progressive abstraction from low-level to high-level features\n",
    "- **Channel Reduction**: Later stages may have fewer channels but richer semantic content\n",
    "- **Resolution Changes**: Spatial resolution decreases while feature complexity increases\n",
    "\n",
    "### Usage Tips:\n",
    "\n",
    "- Use different input images to see how feature maps change based on content\n",
    "- Experiment with different model sizes ('basic', 'small', 'large')\n",
    "- Try enabling `use_lightweight_optimization` to see LWO block effects\n",
    "- Compare feature maps with and without SE attention\n",
    "\n",
    "This visualization helps understand what the STickNet model learns and can guide model improvements and interpretability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
